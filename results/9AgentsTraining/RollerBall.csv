Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
10000,1.4142874,10.0119825708061,0.37286353,0.432194046306505,0.432194046306505,0.1364793,0.24572109,0.00029672202,0.19890736,0.0004946459,1.0
20000,1.3722064,7.87422497785651,0.5670822,0.5980479148181012,0.5980479148181012,0.07928275,0.24813494,0.00029102154,0.1970072,0.00048533516,1.0
30000,1.365529,8.305194805194805,0.8235724,0.8445065176908753,0.8445065176908753,0.035760872,0.24760658,0.00028491855,0.19497287,0.00047536704,1.0
40000,1.3600439,7.359832635983263,0.8779534,0.8994974874371859,0.8994974874371859,0.028333915,0.24971846,0.00027891214,0.19297071,0.00046555648,1.0
50000,1.359122,6.839016653449643,0.9333906,0.957613814756672,0.957613814756672,0.010816426,0.24213177,0.00027297682,0.19099227,0.0004558621,1.0
60000,1.3578749,6.52352501867065,0.95319283,0.9842342342342343,0.9842342342342343,0.0063970448,0.24245985,0.00026707342,0.18902448,0.00044621993,1.0
70000,1.3210224,6.055147058823529,0.96524125,0.9963423555230432,0.9963423555230432,0.0036248055,0.24366824,0.00026112935,0.18704312,0.0004365112,1.0
